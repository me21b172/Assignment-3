{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "451e1f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch \n",
    "from io import open\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d705df52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e4a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Update the function to create mappings to include the special tokens\n",
    "def create_mappings(vocab):\n",
    "    vocab = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN] + sorted(vocab)\n",
    "    word2int = {word: i for i, word in enumerate(vocab)}\n",
    "    int2word = {i: word for word, i in word2int.items()}\n",
    "    return word2int, int2word\n",
    "\n",
    "def wordEncoder(words,encodelist):\n",
    "    n_letters = len(encodelist)\n",
    "    tensor = torch.zeros(len(words), n_letters)\n",
    "    for i,word in enumerate(words):\n",
    "        tensor[i][encodelist[word]] = 1\n",
    "    return tensor\n",
    "    \n",
    "def tokenise(word, wordMap):\n",
    "    return torch.tensor([wordMap[SOS_TOKEN]] + [wordMap(letter) for letter in word] + [wordMap[EOS_TOKEN]], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71419377",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = {\"train\":'mr.translit.sampled.train.tsv',\"val\":'mr.translit.sampled.dev.tsv',\"test\":\"mr.translit.sampled.test.tsv\"}\n",
    "with open(os.path.join(\"lexicons/\",types[\"train\"]), \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "train_data = np.array([[text.split(\"\\t\")[0],text.split(\"\\t\")[1][:-1]] for text in lines if not text.split(\"\\t\")[0] == '</s>'])\n",
    "with open(os.path.join(\"lexicons/\",types[\"val\"]), \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "val_data = np.array([[text.split(\"\\t\")[0],text.split(\"\\t\")[1][:-1]] for text in lines if not text.split(\"\\t\")[0] == '</s>'])\n",
    "with open(os.path.join(\"lexicons/\",types[\"test\"]), \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "test_data = np.array([[text.split(\"\\t\")[0],text.split(\"\\t\")[1][:-1]] for text in lines if not text.split(\"\\t\")[0] == '</s>'])\n",
    "merged_data = np.concatenate((train_data,val_data,test_data))\n",
    "len(merged_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a80d4443",
   "metadata": {},
   "outputs": [],
   "source": [
    "devnagri2int,latinList2int = {letter: idx for idx, letter in enumerate(set(\"\".join(merged_data[:, 0])))},{letter: idx for idx, letter in enumerate(set(\"\".join(merged_data[:, 1])))}\n",
    "int2devnagri,int2latinList = {idx: letter for letter, idx in devnagri2int.items()},{idx: letter for letter, idx in latinList2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3f52409",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[text.split(\"\\t\")[0],text.split(\"\\t\")[1][:-1]] for text in lines if not text.split(\"\\t\")[0] == '</s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5aa985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the vocabularies\n",
    "devnagri2int, int2devnagri = create_mappings(set(\"\".join(merged_data[:, 0])))\n",
    "latin2int, int2latin = create_mappings(set(\"\".join(merged_data[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19098b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangDataset(Dataset):\n",
    "    def __init__(self,type:str):\n",
    "        types = {\"train\":train_data,\"val\":val_data,\"test\":test_data}\n",
    "        data = types[type]\n",
    "        self.X,self.Y,self.X_encoded,self.Y_encoded = [],[],[],[]\n",
    "        for word in data:\n",
    "            self.X.append(word[1])\n",
    "            self.Y.append(word[0])\n",
    "            self.X_encoded.append(tokenise(word[1],latin2int))\n",
    "            self.Y_encoded.append(tokenise(word[0],devnagri2int))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        latin_word= self.X[idx]\n",
    "        devnagri_word = self.Y[idx]\n",
    "        latin_tensor = self.X_encoded[idx]\n",
    "        devnagri_tensor = self.Y_encoded[idx]\n",
    "\n",
    "        return latin_word, devnagri_word, latin_tensor, devnagri_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae68f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6449]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor1 = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "lookup_tensor2 = torch.tensor([word_to_ix[\"world\"]], dtype=torch.long)\n",
    "hello_embed1 = embeds(lookup_tensor1)\n",
    "hello_embed2 = embeds(lookup_tensor2)\n",
    "print(hello_embed2@hello_embed1.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489005d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[156], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43m(\u001b[49m\u001b[43mhello_embed2\u001b[49m\u001b[38;5;129;43m@hello_embed1\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "(hello_embed2@hello_embed1.T)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdc6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size,embed_size, hidden_size, num_layers,nonlinearity, dropout_p=0.1, layer = \"rnn\"):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)  #nn.Embedding(2, 5)   2 words in vocab, 5 dimensional embeddings\n",
    "        \n",
    "        if layer == \"rnn\":\n",
    "            self.cell = nn.RNN(embed_size, hidden_size,num_layers,nonlinearity, batch_first=True) \n",
    "        elif layer == \"gru\":   \n",
    "            self.cell = nn.GRU(embed_size, hidden_size,num_layers, batch_first=True)\n",
    "        elif layer == \"lstm\":\n",
    "            self.cell = nn.LSTM(embed_size, hidden_size,num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.cell(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7bcf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x)\n",
    "        out, (hidden, cell) = self.lstm(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d110073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(encoder, decoder, sentence, eng_word2int, dev_int2word, max_length=15):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Tokenize and encode the sentence\n",
    "        input_tensor = torch.tensor([eng_word2int[word] for word in sentence]\n",
    "                                    + [eng_word2int[EOS_TOKEN]], dtype=torch.long)\n",
    "        input_tensor = input_tensor.view(1, -1).to(DEVICE)  # batch_first=True\n",
    "\n",
    "        # Pass the input through the encoder\n",
    "        _, encoder_hidden, encoder_cell = encoder(input_tensor)\n",
    "\n",
    "        # Initialize the decoder input with the SOS token\n",
    "        decoder_input = torch.tensor([[eng_word2int[SOS_TOKEN]]], dtype=torch.long)  # SOS\n",
    "        # Initialize the hidden state of the decoder with the encoder's hidden state\n",
    "        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
    "\n",
    "        # Decoding the sentence\n",
    "        decoded_words = []\n",
    "        last_word = torch.tensor([[eng_word2int[SOS_TOKEN]]]).to(DEVICE)\n",
    "        for _ in range(max_length):\n",
    "            logits, decoder_hidden, decoder_cell = decoder(last_word, decoder_hidden, decoder_cell)\n",
    "            next_token = logits.argmax(dim=1) # greedy\n",
    "            last_word = torch.tensor([[next_token]]).to(DEVICE)\n",
    "            if next_token.item() == dev_word2int[EOS_TOKEN]:\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(dev_int2word.get(next_token.item()))\n",
    "\n",
    "        return ' '.join(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6aa034",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LangDataset(\"train\")\n",
    "val_dataset = LangDataset(\"val\")\n",
    "test_dataset = LangDataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3fb19f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
